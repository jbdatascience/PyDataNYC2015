{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Word embeddings as a service\n",
    "<br/>\n",
    "<br/>\n",
    "#### François Scharffe ([@lechatpito](http://www.twitter.com/lechatpito)), [3Top Inc.](http://www.3top.com)\n",
    "<br/>\n",
    "<br/>\n",
    "#### PyData NYC 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline of the talk\n",
    "\n",
    "* What is 3Top?\n",
    "* What are word embeddings?\n",
    "* How to implement a simple recommendation system for 3Top categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rank Anything, Rank Everything\n",
    "* 3Top is a ranking and recommendation platform\n",
    "* Rankings convey more information that star ratings\n",
    "    * Who cares about 3 stars or less? I just want the best stuff\n",
    "    * I'd rather trust my friends than reading through reviews\n",
    "    * If I have more than 3 items to rank, I can probably use a more precise category\n",
    "\n",
    "\n",
    "* Not yet launched, but the site is up\n",
    "\n",
    "Let's take a look at http://www.3top.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/1980s Movies.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Places\n",
    "<img src=\"imgs/Gyms for Students near Lower East Side.png\">\n",
    "http://www.3top.com/category/1138/gyms-for-students-near-lower-east-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " * Movies\n",
    "<img src=\"imgs/Movies About Wall Street.png\">\n",
    "http://www.3top.com/category/142/movies-about-wall-street"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Anything really\n",
    "<img src=\"imgs/Foods Named After People.png\">\n",
    "http://www.3top.com/category/765/foods-named-after-people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data & knowledge engineering at 3Top\n",
    "Building a solid data engineering architecture before launching the site.\n",
    "* Natural language processing pipeline\n",
    "    * Parsing categories\n",
    "    * Detecting named entities, locations\n",
    "* A large knowledge graph backed by an ontology\n",
    "* An itemization pipeline \n",
    "    * matching free text items to entities in the knowledge graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/Knowledge Graph.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Category recommendation\n",
    "\n",
    "How are we going to build a simple recommendation system without having any significant number of user, categories or rankings?\n",
    "\n",
    "Note the impressive figures:\n",
    " * Number of Users: 316\n",
    " * Number of Rankings: 2123\n",
    " * Number of Categories: 1316\n",
    "\n",
    "        Wow ! ;)\n",
    " \n",
    "Feel free to add a few rankings: http://www.3top.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word embeddings ?\n",
    "Who hasn't heard about word2vec?\n",
    "\n",
    "Word embeddings allow to represent words in a high dimensional space in a way that words appearing in the same context will be close in that space.\n",
    "* Dimensionality of the space is not that high, typically around 100 dimensions.\n",
    "* Word embeddings is a language modeling method, more precisely a distributed vector representation of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compared to Bag of words:\n",
    " * Dimensionality is low and constant\n",
    " * Depending on the technique, partially learned models give partially good results (5)\n",
    "\n",
    "Compared to topic modeling:\n",
    " * Better granularity, the base element is a word\n",
    " * Phrases vector can also be learnt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is it good at:\n",
    " * Modeling similarity between words\n",
    "       sim(tomato, beefsteak) < sim(apple, tomato) < sim(pear, apple)\n",
    " * Allows algebric operations on word vectors\n",
    "       v(Paris) - v(France) ~= v(Berlin) - v(Germany)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "Examples here are using a small GloVe model (100d, 400k vocab, trained on Wikipedia and Gigaword)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec().load_word2vec_format(\"./glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'monty', 0.6886237859725952),\n",
       " (u'php', 0.586538553237915),\n",
       " (u'perl', 0.5784406661987305),\n",
       " (u'cleese', 0.5446674823760986),\n",
       " (u'flipper', 0.5112984776496887),\n",
       " (u'ruby', 0.5066927671432495),\n",
       " (u'spamalot', 0.505638837814331),\n",
       " (u'javascript', 0.5030568838119507),\n",
       " (u'reticulated', 0.4983375668525696),\n",
       " (u'monkey', 0.49764129519462585)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"python\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'perl', 0.5658619999885559),\n",
       " (u'scripting', 0.559501588344574),\n",
       " (u'scripts', 0.5469149351119995),\n",
       " (u'php', 0.5461974740028381),\n",
       " (u'language', 0.5350533127784729)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar_cosmul(positive=[\"python\", \"programming\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'scorpion', 0.5413044095039368),\n",
       " (u'snakes', 0.5263831615447998),\n",
       " (u'snake', 0.5222328901290894),\n",
       " (u'spider', 0.5214570164680481),\n",
       " (u'marsupial', 0.517005205154419)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar_cosmul(positive=[\"python\", \"venomous\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The classical example: \n",
    "                \n",
    "                v(king) - v(man) + v(woman) -> v(queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.8964556455612183),\n",
       " (u'monarch', 0.8495977520942688),\n",
       " (u'throne', 0.8447030782699585),\n",
       " (u'princess', 0.8371668457984924),\n",
       " (u'elizabeth', 0.835679292678833),\n",
       " (u'daughter', 0.8348594903945923),\n",
       " (u'prince', 0.8230059742927551),\n",
       " (u'mother', 0.8154449462890625),\n",
       " (u'margaret', 0.8147734999656677),\n",
       " (u'father', 0.8100854158401489)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar_cosmul(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a model\n",
    "\n",
    "* Very easy once you have a clean corpus\n",
    "* Great tools in Python\n",
    "  * Tutorial on training a model using Gensim: http://rare-technologies.com/word2vec-tutorial/\n",
    "  * Radim Řehůřek gave a talk last year at PyData Berlin about optimizations in Cython: https://www.youtube.com/watch?v=vU4TlwZzTfU\n",
    "  * For GLoVe https://github.com/maciejkula/glove-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gensim word2vec implementation specifics:\n",
    " * Training time ~ 8hours on a 8 proc/8 threads to learn 600 dimensions on a <em>1.9B words</em> corpus\n",
    " * Memory requirements depends on the vocabulary size and on the number of dimensions:\n",
    "       3 matrices * 4 bytes (float) * |dimensions| * |vocabulary|\n",
    "\n",
    "The [GloVe implementation in Python](https://github.com/maciejkula/glove-python/) takes half the time but has a quadratic memory size instead of linear. Check pull requests for memory optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A good think to know: a bigger training set does improve the quality of the model, <em>even for specialized tasks</em>.\n",
    "\n",
    "As a consequence, you probably want to use a huge corpus. Good models are available.\n",
    "\n",
    "Building you own model can be useful when you want to find out about the properties of your corpus, or you want to compare different corpora together. For examples evolution of language in a newspaper during different periods of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding a model\n",
    "\n",
    "From: https://github.com/3Top/word2vec-api/\n",
    "\n",
    "| Model file | Number of dimensions | Corpus (size)| Vocabulary size | Author | Architecture | Training Algorithm | Context window - size | Web page |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| [Google News](GoogleNews-vectors-negative300.bin.gz) | 300 |Google News (100B) | 3M | Google | word2vec | negative sampling | BoW - ~5| [link](http://code.google.com/p/word2vec/) |\n",
    "| [Freebase IDs](https://docs.google.com/file/d/0B7XkCwpI5KDYaDBDQm1tZGNDRHc/edit?usp=sharing) | 1000 | Gooogle News (100B) | 1.4M | Google | word2vec, skip-gram | ? | BoW - ~10 | [link](http://code.google.com/p/word2vec/) |\n",
    "| [Freebase names](https://docs.google.com/file/d/0B7XkCwpI5KDYeFdmcVltWkhtbmM/edit?usp=sharing) | 1000 | Gooogle News (100B) | 1.4M | Google | word2vec, skip-gram | ? | BoW - ~10 | [link](http://code.google.com/p/word2vec/) |\n",
    "| [Wikipedia+Gigaword 5](http://nlp.stanford.edu/data/glove.6B.zip) | 50/100/200/300 | Wikipedia+Gigaword 5 (6B) | 400,000 | GloVe | GloVe | AdaGrad | 10+10 | [link](http://nlp.stanford.edu/projects/glove/) |\n",
    "| [Common Crawl 42B](http://nlp.stanford.edu/data/glove.42B.300d.zip) | 300 | Common Crawl (42B) | 1.9M | GloVe | GloVe | GloVe | AdaGrad | [link](http://nlp.stanford.edu/projects/glove/) |\n",
    "| [Common Crawl 840B](http://nlp.stanford.edu/data/glove.840B.300d.zip) | 300 | Common Crawl (840B) | 2.2M | GloVe | GloVe | GloVe | AdaGrad | [link](http://nlp.stanford.edu/projects/glove/) |\n",
    "| [Twitter (2B Tweets)](http://www-nlp.stanford.edu/data/glove.twitter.27B.zip) | 25/50/100/200 | Twitter (27B) | ? | GloVe | GloVe | GloVe | AdaGrad | [link](http://nlp.stanford.edu/projects/glove/) |\n",
    "| [Wikipedia dependency](http://u.cs.biu.ac.il/~yogo/data/syntemb/deps.words.bz2) | 300 | Wikipedia (?) | 174,015 | Levy \\& Goldberg | word2vec modified | word2vec | syntactic dependencies | [link](https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/) |\n",
    "| [DBPedia vectors](https://github.com/idio/wiki2vec/raw/master/torrents/enwiki-gensim-word2vec-1000-nostem-10cbow.torrent) | 1000 | Wikipedia (?) | ? | wiki2vec | word2vec | word2vec, skip-gram | BoW, 10 | [link](https://github.com/idio/wiki2vec#prebuilt-models) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a recommendation engine for 3Top categories\n",
    "\n",
    "By combining word vectors, we build category vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def build_category_vector(category):    \n",
    "    pass  # Get the postags\n",
    "    vector = []\n",
    "    for tag in postags:\n",
    "        if tag.tagged in ['NN', 'NNS', 'JJ', 'NNP', 'NNPS', 'NNDBN', 'VBG', 'CD']:  # Only keep meaningful words\n",
    "            try:\n",
    "                v = word2vec(tag.tagValue)  # Get the word vector\n",
    "                if v.any():\n",
    "                    vector.append(v)\n",
    "            except:\n",
    "                logger.debug(\"Word not found in corpus: %s\" % tag.tagValue)\n",
    "            tagset.add(tag.tagValue)\n",
    "    if vector:\n",
    "        return matutils.unitvec(np.array(vector).mean(axis=0))  # Average the vector\n",
    "    else:\n",
    "        return np.empty(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We store those vectors in a category space, and at page load time compute the most similar categories for a given category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let us look at the similarity method.\n",
    "\n",
    "    sim(c1, c2) = cos(v(c1), v(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG Category space size (as found in the cache): 1125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belgian Trappist Beers \n",
      "Belgian Beer Cafe in NYC\n",
      "Dark and Stormy Cocktail in NYC\n",
      "Brands of Ginger Beer\n",
      "Pink Drinks\n"
     ]
    }
   ],
   "source": [
    "cs = CategorySimilarity()\n",
    "# print(Category.objects.all().count())\n",
    "category = Category.objects.get(category=u\"Blue-collar beers that come in a can\")\n",
    "_ = [print(c) for c in cs.most_similar_categories(category, n=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian Restaurants in NY\n",
      "Restaurants in Nyc\n",
      "NYC Mexican Restaurants \n",
      "Romanian Restaurants in NYC\n",
      "Thai Restaurants in NYC\n"
     ]
    }
   ],
   "source": [
    "category = Category.objects.get(category=u\"Italian Restaurants in NYC.\")\n",
    "_ = [print(c) for c in cs.most_similar_categories(category, n=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quentin Tarantino Movies\n",
      "Martin Scorsese Films.\n",
      "Movies Starring Creepy Children\n",
      "Tim Burton Movies\n",
      "Movies Starring Sean Penn\n",
      "Pixar Movies\n",
      "Godfather Movies\n",
      "Berlin Indie Movie Theaters\n",
      "Kubrick Movies\n",
      "Harry Potter Movies\n"
     ]
    }
   ],
   "source": [
    "category = Category.objects.get(category=u\"Coen Brothers Movies\")\n",
    "_ = [print(c) for c in cs.most_similar_categories(category, n=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our recommendation system uses the Common Crawl 42B words, 300 dimensions model trained with GloVe.\n",
    "\n",
    "It takes around 6GB in memory ... and this is a problem:\n",
    "\n",
    "We run a Django server and 8 celery workers on an EC2 T2 Micro... That would be a lot of memory\n",
    "for that poor instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A word embedding service\n",
    "\n",
    "* We separate the word embedding model as a service\n",
    "* Simple Flask server with a few primitives:\n",
    "    * `curl http://127.0.0.1:5000/word2vec/similarity?w1=Python&w2=Java`\n",
    "    * `curl http://127.0.0.1:5000/word2vec/n_similarity?ws1=Python&ws1=programming&ws2=Java`\n",
    "    * `curl http://127.0.0.1:5000/word2vec/model?word=Python`\n",
    "    * `curl http://127.0.0.1:5000/word2vec/most_similar?positive=king&positive=queen&negative=man`\n",
    "* Easy to setup: \n",
    "    * `python word2vec-api --model path/to/the/model [--host host --port 1234]`\n",
    "* Get it at https://github.com/3Top/word2vec-api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/3Top Production Infrastructure 11-2014.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caching the vector space\n",
    "\n",
    "As the number of categories increase we do not want to hit the database every time a recommendation is needed (every page access). Category vectors size is actually significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "print(np.fromstring(category.vector).nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and for 1125 categories the space becomes large (~2MB)\n"
     ]
    }
   ],
   "source": [
    "print(u\"... and for {} categories the space becomes large (~{}MB)\".format(len(cs.category_space.syn0), cs.category_space.syn0.nbytes/1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We store vectors with the category object in MySQL, using a base64 encoding of the numpy object. Let's look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oNl7j9l8hr/2FoHhEbSyv+GALkJ6JVU/rxm5pueouL80aF72QLiivzr0z1WKaZM/i3M5FvqQmD+hDmIs7fitv+lL6cLFSbM/lSwwkBxv0D/sA+FgTxmhP5+lJZPGVLE/q8pBj07ukT9OceKjxl2jv4s4cA7RJIc/JxVUF8afnr/RQcXciUyCP+M4N3mbtrS/Ngwo85uUor/+4vqargCyP7YnHbTSv3W/MHjrh6iHsr/1vkzSmI+yP+bsS7E0B5u/JJ5iJAUNoT//xo0IJ3inP5/BwCfgWZ2/Q2r8q9Fuir/KdAOAr0OQPwzGTnUXU5i/9uQD77+xuD/1QKbEaDWjvyqfSePd7XE/3RLqJXiOrz8ZyEDICd2UP2beFLiqPZy/rIb+8sFgqr+ILyc3/5yoP5pL25IahpQ/4WpgCeuNZ7/ley+Tl9ygP+knz2odUHo/yBSdc5+Klj+GLgrafftvP2yiB76KBY2/z0RqB+1ri7+THdXBVVKvPzwZ2X+2HaA/oOThsHeidL/O7w8+bummv8Z8XCqeYas/WzQpioG6qr+JaauGrie2P7+8NmNBN5o/0Ji6XFJMpj/xAtoHvg9PPyr3OOBXVaG/M2aCbN8dv79pANKgDzNrPy4XXBNVi4S/WBuYYjWZlD8T/W3jLbOJPy3xHNTzarQ/MoOWx7aZtz/RDMwbryievwA5kQgazaO/3Ep0jVo1rD+ns3oJ3iWUv4TlEPcAnJy/dHNcwygjnr/cMGYNKPbCP5E06afPWa6/mUHAC+8mjj+NwgyjQFB5v6ffLvduuai/kBntVvsdpb8Yg3KzY/qev9r5son3VJg/h06aD0/IuD8NMHm7jGViv7o8zQzPd5U/esP4Ax6BrL8TOZuX+qGpP1WHNPzdQH0/7HXRMAqXmr9G8pkwjbenv3RvQppal7i/E5jWmLXSp792VpPxJeOjPyEKhEhl324/1E00QnHdvr9Mg0Fohd+cP6UAj0X5R5g/2umwTF42...\n"
     ]
    }
   ],
   "source": [
    "print(category._vector[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# a property method takes care of the decoding\n",
    "def get_vector(self):\n",
    "        return base64.b64decode(self._vector)\n",
    "\n",
    "def set_vector(self, value):\n",
    "    encoded = base64.b64encode(value)\n",
    "    self._vector = encoded\n",
    "        \n",
    "vector = property(get_vector, set_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01098032, -0.07306015,  0.00129067, -0.09632728, -0.03656199,\n",
       "        0.01895729,  0.02399054, -0.05853978,  0.07534443,  0.25678171,\n",
       "        0.03339623,  0.06769982,  0.01751063, -0.03782483,  0.01130069,\n",
       "       -0.02990636,  0.00893505, -0.08091137, -0.03629005,  0.07032291,\n",
       "       -0.00530989, -0.07238248,  0.07250362, -0.02639468,  0.03330246,\n",
       "        0.04583857, -0.02866316, -0.01290668,  0.0158832 , -0.02375447,\n",
       "        0.09646225, -0.03751686,  0.00437724,  0.06163383,  0.02037444,\n",
       "       -0.02757899, -0.05151945,  0.04807279,  0.02004282, -0.00287529,\n",
       "        0.03293298,  0.00642406,  0.02201318,  0.0039041 , -0.01417073,\n",
       "       -0.01338945,  0.06117504,  0.03147669, -0.00503775, -0.04474968,\n",
       "        0.05347914, -0.05220418,  0.086543  ,  0.02560141,  0.04355104,\n",
       "        0.00094792, -0.03385424, -0.12154957,  0.00332025, -0.01003138,\n",
       "        0.02011569,  0.01254879,  0.07975696,  0.09218924, -0.02945207,\n",
       "       -0.03867418,  0.05509456, -0.0196757 , -0.02793886, -0.029431  ,\n",
       "        0.1481371 , -0.05927895,  0.0147227 , -0.00618005, -0.04828975,\n",
       "       -0.04124437, -0.03025203,  0.02376162,  0.09680647, -0.00224569,\n",
       "        0.02096485, -0.05567259,  0.05006393,  0.00714194, -0.0259668 ,\n",
       "       -0.04632226, -0.09605948, -0.04652946,  0.03884238,  0.00376863,\n",
       "       -0.12056644,  0.02819642,  0.02371206,  0.08286085,  0.08104846,\n",
       "       -0.03060514, -0.0313298 , -0.00715603, -0.05278924,  0.0031662 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fromstring(category.vector)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In order to avoid issuing a few thousand SQL queries every time a page is loaded we use Memcache to store the category space. \n",
    "* As the space is larger than a MB we store each vector with its own key (the category Id). They share a common key prefix.\n",
    "* Here we directly store the numpy vectors through the Gensim API\n",
    "* A separate key is used for the vocabulary indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def set_space_cache(space):\n",
    "        sim.set(VOC, space.vocab)\n",
    "        sim.set(IDX, space.index2word)\n",
    "        sim.set_many({\"{0}-{1}\".format(VEC, i): space.syn0[i] for i in range(len(space.vocab))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This also allows to add a category vector to the space without having to rebuild it. Simply by stacking its vector in the cache and updating the cached space indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def add_last_vector_to_space_cache(space):\n",
    "        sim.set(VOC, space.vocab)\n",
    "        sim.set(IDX, space.index2word)\n",
    "        sim.set(\"{}-{}\".format(VEC, len(space.vocab)-1), space.syn0[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Updates\n",
    "* Each process gets its own copy of the vector space.\n",
    "* Whenver a process adds a category, it also updates the space. \n",
    "* Django signals are used to tell other processes to reload the space from cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Work in progress\n",
    "* We are about to add a few 100k generated categories\n",
    "* The category space will becode large in memory: 8 workers \\* 2.4 kb \\* 100000 categories = 1,9 GB \n",
    "* Including entity vectors would improve results for names, places, etc.\n",
    "* Training a specialized corpus using categories scraped all over the web\n",
    "* Train a phrase2Vec model on these categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "### Tutorials & Applications\n",
    "* Instagram: http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji\n",
    "* Word embeddings and RNNs: http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n",
    "* Word2vec gensim tutorial: http://rare-technologies.com/word2vec-tutorial/\n",
    "* Clothing style search: http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/\n",
    "* In digital humanities: http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html\n",
    "* In digital humanities, application to gender studies: http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "### Academic Papers\n",
    "* Le, Quoc V., and Tomas Mikolov. \"Distributed representations of sentences and documents.\" arXiv preprint arXiv:1405.4053 (2014).\n",
    "* JeffreyPennington, RichardSocher, and ChristopherD Manning. \"Glove: Global vectors for word representation.\" (2014).\n",
    "* Levy, Omer, and Yoav Goldberg. \"Dependencybased word embeddings.\" Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Vol. 2. 2014.\n",
    "* Goldberg, Yoav, and Omer Levy. \"word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method.\" arXiv preprint arXiv:1402.3722 (2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-f087ca1d6988>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-f087ca1d6988>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Thank you !\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Thank you !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
